# Neural Network Regularization Comparison

This repository contains code demonstrating the comparison of various regularization techniques in neural networks using the MNIST dataset.

## Overview

In this project, we explore the effects of different regularization techniques on the performance of neural networks trained on the MNIST dataset. The regularization techniques examined include:

- L2 regularization
- L1 regularization
- Dropout regularization

We compare the performance of neural networks trained with each regularization method against a baseline model without regularization.

## Files

- `1ML_10-04-24.ipynb`: Jupyter Notebook containing the code for training and evaluating neural networks with different regularization techniques.
- `README.md`: This README file providing an overview of the project.

## Usage

1. Clone the repository:

```bash
git clone https://github.com/mudit2004/Machine-Learning-Techniques.git
```

2. Open and run the `1ML_10-04-24.ipynb` notebook in a Jupyter environment. Make sure to have necessary dependencies installed.

3. Follow the instructions in the notebook to train and evaluate neural networks with L2, L1, and dropout regularization techniques.

## Results

The notebook provides visual comparisons of the performance (accuracy and loss) of neural networks with different regularization methods against the baseline model without regularization.

## Dependencies

The code requires the following dependencies:

- Python 3
- NumPy
- TensorFlow
- Keras
- Matplotlib
- scikit-learn

You can install the dependencies using pip:

```bash
pip install numpy tensorflow keras matplotlib scikit-learn
```

## Contributing

Contributions to improve the project are welcome! If you find any issues or have suggestions for enhancements, feel free to open an issue or submit a pull request.

## License

This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.
